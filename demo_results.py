# -*- coding: utf-8 -*-
"""
æ¼”ç¤ºæ”¹è¿›åçš„ç»“æœæ³¨é‡ŠåŠŸèƒ½
"""

import sys
import os
sys.path.append(os.path.dirname(__file__))

from utils.result_formatter import format_evaluation_result, print_formatted_result
from utils.metrics import EvaluationMetrics

def create_demo_result():
    """åˆ›å»ºä¸€ä¸ªæ¼”ç¤ºç”¨çš„è¯„ä¼°ç»“æœ"""
    return {
        "idx": "demo-001",
        "query": "ç§¯åˆ†å•†å“åˆ—è¡¨æ ·å¼",
        "category": "style",
        "weight": 1.0,
        "description": "æµ‹è¯•ç§¯åˆ†å…‘æ¢é¡µé¢å•†å“åˆ—è¡¨çš„CSSæ ·å¼æ£€ç´¢",
        
        # æ–°è¯„ä¼°æ¡†æ¶æŒ‡æ ‡
        "total_score": 0.72,
        "relevance": 0.8,
        "completeness": 0.6,
        "usability": 0.9,
        
        # æ¡†æ¶è¯¦æƒ…
        "framework_metrics": {
            "total_score": 0.72,
            "relevance": 0.8,
            "completeness": 0.6,
            "usability": 0.9,
            "details": {
                "relevant_in_top_k": 8,
                "total_relevant": 2,
                "k": 10,
                "mrr": 0.9,
                "weights": {"relevance": 0.5, "completeness": 0.3, "usability": 0.2}
            }
        },
        
        # ä¼ ç»ŸæŒ‡æ ‡ (ä¿ç•™)
        "precision": 0.6,
        "recall": 0.8,
        "f1_score": 0.686,
        
        # è·¯å¾„åŒ¹é…
        "path_matching": {
            "total_score": 0.75,
            "exact_matches": 1,
            "partial_matches": 1,
            "extension_matches": 0,
            "total_expected": 2
        },
        
        # Top-Kå‡†ç¡®ç‡
        "top_k_accuracy": {
            1: 1.0,
            3: 0.8,
            5: 0.6,
            10: 0.5
        },
        
        # åˆ†æ•°åˆ†æ
        "score_analysis": {
            "max_score": 0.85,
            "min_score": 0.32,
            "avg_score": 0.58,
            "std_score": 0.18,
            "score_gap": 0.53,
            "top_3_avg": 0.72
        },
        
        # é«˜çº§æŒ‡æ ‡
        "mrr": 0.8,
        "ndcg": 0.75,
        "diversity": 0.6,
        
        # åŸå§‹æ•°æ®
        "expected_results": [
            {"path": "pages\\points\\exchange.vue", "relevance_score": 1.0},
            {"path": "pages\\coupon\\list.vue", "relevance_score": 0.7}
        ],
        "actual_results": [
            {"path": "pages\\points\\exchange.vue", "score": 0.85},
            {"path": "pages\\coupon\\list.vue", "score": 0.72},
            {"path": "components\\goods-card.vue", "score": 0.58},
            {"path": "api\\goods.js", "score": 0.32}
        ],
        "total_results": 4,
        
        # å…ƒæ•°æ®
        "timestamp": 1640995200.0,
        "success": True
    }

def demo_formatted_results():
    """æ¼”ç¤ºæ ¼å¼åŒ–ç»“æœåŠŸèƒ½"""
    print("ğŸ¯ ä»£ç æ£€ç´¢è¯„ä¼°ç»“æœæ³¨é‡Šæ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºæ¼”ç¤ºæ•°æ®
    demo_result = create_demo_result()
    
    print("\nğŸ“‹ åŸå§‹ç»“æœ (ç®€åŒ–ç‰ˆ):")
    print(f"  æŸ¥è¯¢: {demo_result['query']}")
    print(f"  ğŸ† ç»¼åˆè¯„åˆ†: {demo_result['total_score']:.3f}")
    print(f"  ğŸ“Š ç›¸å…³æ€§: {demo_result['relevance']:.3f}")
    print(f"  ğŸ“ˆ å…¨é¢æ€§: {demo_result['completeness']:.3f}")
    print(f"  ğŸ¯ å¯ç”¨æ€§: {demo_result['usability']:.3f}")
    print(f"  (ä¼ ç»ŸF1åˆ†æ•°: {demo_result['f1_score']:.3f})")
    
    print("\n" + "="*60)
    print("ğŸ”„ æ­£åœ¨æ·»åŠ è¯¦ç»†æ³¨é‡Š...")
    
    # æ ¼å¼åŒ–ç»“æœ
    formatted_result = format_evaluation_result(demo_result)
    
    print("\nğŸ“Š æ ¼å¼åŒ–åçš„è¯¦ç»†ç»“æœ:")
    print_formatted_result(formatted_result, show_details=True)
    
    print("\n" + "="*60)
    print("ğŸ’¡ æ³¨é‡ŠåŠŸèƒ½è¯´æ˜:")
    print("âœ… æ¯ä¸ªæŒ‡æ ‡éƒ½æœ‰è¯¦ç»†çš„å«ä¹‰è§£é‡Š")
    print("âœ… æä¾›è®¡ç®—å…¬å¼å’Œå–å€¼èŒƒå›´")
    print("âœ… æ ¹æ®åˆ†æ•°ç»™å‡ºè¯„ä»·ç­‰çº§å’Œé¢œè‰²æ ‡è¯†")
    print("âœ… åŒ…å«å…·ä½“çš„æ”¹è¿›å»ºè®®")
    print("âœ… è·¯å¾„åŒ¹é…æœ‰è¯¦ç»†çš„åˆ†ç±»ç»Ÿè®¡")
    print("âœ… é«˜çº§æŒ‡æ ‡æœ‰ä¸“ä¸šçš„è§£é‡Šè¯´æ˜")
    
    return formatted_result

def demo_different_scores():
    """æ¼”ç¤ºä¸åŒåˆ†æ•°ç­‰çº§çš„æ˜¾ç¤ºæ•ˆæœ"""
    print("\n\nğŸ¨ ä¸åŒè¯„ä»·ç­‰çº§æ¼”ç¤º")
    print("=" * 60)
    
    test_scores = [
        {"name": "ä¼˜ç§€æ¡ˆä¾‹", "total": 0.85, "relevance": 0.9, "completeness": 0.8, "usability": 0.9},
        {"name": "è‰¯å¥½æ¡ˆä¾‹", "total": 0.65, "relevance": 0.7, "completeness": 0.6, "usability": 0.7},
        {"name": "ä¸€èˆ¬æ¡ˆä¾‹", "total": 0.45, "relevance": 0.5, "completeness": 0.4, "usability": 0.5},
        {"name": "è¾ƒå·®æ¡ˆä¾‹", "total": 0.25, "relevance": 0.3, "completeness": 0.2, "usability": 0.3},
        {"name": "å¾ˆå·®æ¡ˆä¾‹", "total": 0.05, "relevance": 0.1, "completeness": 0.05, "usability": 0.1}
    ]
    
    for test_case in test_scores:
        demo_result = create_demo_result()
        demo_result["query"] = test_case["name"]
        demo_result["total_score"] = test_case["total"]
        demo_result["relevance"] = test_case["relevance"]
        demo_result["completeness"] = test_case["completeness"]
        demo_result["usability"] = test_case["usability"]
        
        formatted_result = format_evaluation_result(demo_result)
        
        print(f"\nğŸ“Š {test_case['name']}:")
        if "framework_explanation" in formatted_result:
            framework = formatted_result["framework_explanation"]
            total_info = framework["total_score"]
            interp = total_info["interpretation"]
            print(f"  {interp['color']} ç»¼åˆè¯„åˆ†: {total_info['value']:.3f} ({interp['level']})")
            print(f"  ğŸ’¡ å»ºè®®: {interp.get('advice', 'æ— ç‰¹æ®Šå»ºè®®')}")

def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("ğŸš€ ä»£ç æ£€ç´¢è¯„ä¼°ç»“æœæ³¨é‡Šç³»ç»Ÿæ¼”ç¤º")
    print("=" * 80)
    
    # æ¼”ç¤ºåŸºæœ¬æ ¼å¼åŒ–åŠŸèƒ½
    formatted_result = demo_formatted_results()
    
    # æ¼”ç¤ºä¸åŒåˆ†æ•°ç­‰çº§
    demo_different_scores()
    
    print("\n\nğŸ‰ æ¼”ç¤ºå®Œæˆ!")
    print("=" * 80)
    print("ğŸ“š ç°åœ¨æ‚¨å¯ä»¥:")
    print("  1. åœ¨å®é™…è¯„ä¼°ä¸­çœ‹åˆ°è¿™äº›è¯¦ç»†æ³¨é‡Š")
    print("  2. æ›´å®¹æ˜“ç†è§£æ¯ä¸ªæŒ‡æ ‡çš„å«ä¹‰")
    print("  3. æ ¹æ®é¢œè‰²å’Œç­‰çº§å¿«é€Ÿåˆ¤æ–­æ€§èƒ½")
    print("  4. è·å¾—å…·ä½“çš„æ”¹è¿›å»ºè®®")
    print("\nğŸ’¡ æç¤º: è¿è¡Œ 'python run_evaluation.py' æŸ¥çœ‹å®Œæ•´è¯„ä¼°çš„æ³¨é‡Šæ•ˆæœ")

if __name__ == "__main__":
    main() 