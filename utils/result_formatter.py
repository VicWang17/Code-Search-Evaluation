# -*- coding: utf-8 -*-
"""
è¯„ä¼°ç»“æœæ ¼å¼åŒ–å·¥å…·
ç”¨äºä¸ºè¯„ä¼°ç»“æœæ·»åŠ è¯¦ç»†çš„æ³¨é‡Šå’Œè¯´æ˜
"""

def format_evaluation_result(result):
    """
    æ ¼å¼åŒ–å•ä¸ªè¯„ä¼°ç»“æœï¼Œæ·»åŠ è¯¦ç»†æ³¨é‡Š
    
    Args:
        result: è¯„ä¼°ç»“æœå­—å…¸
        
    Returns:
        dict: åŒ…å«æ³¨é‡Šçš„æ ¼å¼åŒ–ç»“æœ
    """
    if not result.get("success", False):
        return result
    
    # æ·»åŠ æŒ‡æ ‡è§£é‡Š
    formatted_result = result.copy()
    
    # æ–°è¯„ä¼°æ¡†æ¶æŒ‡æ ‡ (ä¸»è¦)
    formatted_result["new_framework_metrics"] = {
        "total_score": {
            "value": result.get("total_score", 0.0),
            "description": "æ–°æ¡†æ¶ç»¼åˆè¯„åˆ† - ç›¸å…³æ€§ã€å…¨é¢æ€§ã€å¯ç”¨æ€§çš„åŠ æƒå¹³å‡",
            "weight_breakdown": "ç›¸å…³æ€§50% + å…¨é¢æ€§30% + å¯ç”¨æ€§20%",
            "range": "0.0-1.0",
            "interpretation": get_total_score_interpretation(result.get("total_score", 0.0))
        },
        "relevance": {
            "value": result.get("relevance", 0.0),
            "description": "æ–°æ¡†æ¶ç›¸å…³æ€§ - å‰kä¸ªç»“æœä¸­ç›¸å…³ç»“æœçš„æ¯”ä¾‹",
            "formula": "å‰kä¸ªç»“æœä¸­ç›¸å…³æ•° / k",
            "range": "0.0-1.0",
            "interpretation": get_score_interpretation(result.get("relevance", 0.0))
        },
        "completeness": {
            "value": result.get("completeness", 0.0),
            "description": "æ–°æ¡†æ¶å…¨é¢æ€§ - å‰kä¸ªç»“æœæ‰¾åˆ°çš„ç›¸å…³ç»“æœå æ€»ç›¸å…³ç»“æœçš„æ¯”ä¾‹",
            "formula": "å‰kä¸ªç»“æœä¸­ç›¸å…³æ•° / æ€»ç›¸å…³æ•°",
            "range": "0.0-1.0",
            "interpretation": get_score_interpretation(result.get("completeness", 0.0))
        },
        "usability": {
            "value": result.get("usability", 0.0),
            "description": "æ–°æ¡†æ¶å¯ç”¨æ€§ - ç¬¬ä¸€ä¸ªç›¸å…³ç»“æœçš„ä½ç½®å€’æ•°(MRR)",
            "formula": "1 / ç¬¬ä¸€ä¸ªç›¸å…³ç»“æœçš„æ’å",
            "range": "0.0-1.0",
            "interpretation": get_score_interpretation(result.get("usability", 0.0))
        }
    }
    
    # è·¯å¾„åŒ¹é…æ³¨é‡Š
    if "path_matching" in result:
        path_matching = result["path_matching"]
        formatted_result["path_matching_explanation"] = {
            "total_score": {
                "value": path_matching.get("total_score", 0.0),
                "description": "è·¯å¾„åŒ¹é…æ€»åˆ† - ç»¼åˆè€ƒè™‘ç²¾ç¡®ã€éƒ¨åˆ†å’Œæ‰©å±•ååŒ¹é…",
                "formula": "(ç²¾ç¡®åŒ¹é…Ã—1.0 + éƒ¨åˆ†åŒ¹é…Ã—0.7 + æ‰©å±•ååŒ¹é…Ã—0.3) / æœŸæœ›ç»“æœæ•°",
                "interpretation": get_score_interpretation(path_matching.get("total_score", 0.0))
            },
            "match_details": {
                "exact_matches": {
                    "count": path_matching.get("exact_matches", 0),
                    "description": "æ–‡ä»¶è·¯å¾„å®Œå…¨ç›¸åŒçš„åŒ¹é…æ•°",
                    "weight": "1.0 (æœ€é«˜æƒé‡)"
                },
                "partial_matches": {
                    "count": path_matching.get("partial_matches", 0),
                    "description": "æ–‡ä»¶åç›¸åŒæˆ–è·¯å¾„æœ‰é‡å çš„åŒ¹é…æ•°",
                    "weight": "0.7"
                },
                "extension_matches": {
                    "count": path_matching.get("extension_matches", 0),
                    "description": "æ–‡ä»¶æ‰©å±•åç›¸åŒçš„åŒ¹é…æ•°",
                    "weight": "0.3"
                }
            }
        }
    
    # Top-Kå‡†ç¡®ç‡æ³¨é‡Š
    if "top_k_accuracy" in result:
        top_k = result["top_k_accuracy"]
        formatted_result["top_k_explanation"] = {}
        for k, accuracy in top_k.items():
            formatted_result["top_k_explanation"][f"top_{k}"] = {
                "value": accuracy,
                "description": f"å‰{k}ä¸ªç»“æœä¸­ç›¸å…³ç»“æœçš„æ¯”ä¾‹",
                "interpretation": get_score_interpretation(accuracy)
            }
    
    # åˆ†æ•°åˆ†ææ³¨é‡Š
    if "score_analysis" in result:
        score_analysis = result["score_analysis"]
        formatted_result["score_analysis_explanation"] = {
            "avg_score": {
                "value": score_analysis.get("avg_score", 0.0),
                "description": "æ£€ç´¢ç»“æœçš„å¹³å‡åˆ†æ•°",
                "interpretation": get_retrieval_score_interpretation(score_analysis.get("avg_score", 0.0))
            },
            "max_score": {
                "value": score_analysis.get("max_score", 0.0),
                "description": "æ£€ç´¢ç»“æœçš„æœ€é«˜åˆ†æ•°",
                "interpretation": "åæ˜ æœ€ç›¸å…³ç»“æœçš„åŒ¹é…ç¨‹åº¦"
            },
            "score_gap": {
                "value": score_analysis.get("score_gap", 0.0),
                "description": "æœ€é«˜åˆ†å’Œæœ€ä½åˆ†çš„å·®è·",
                "interpretation": "å·®è·å¤§è¯´æ˜ç»“æœè´¨é‡å·®å¼‚æ˜æ˜¾"
            }
        }
    
    # é«˜çº§æŒ‡æ ‡æ³¨é‡Š
    if "mrr" in result:
        formatted_result["advanced_metrics_explanation"] = {
            "mrr": {
                "value": result.get("mrr", 0.0),
                "description": "å¹³å‡å€’æ•°æ’å - ç¬¬ä¸€ä¸ªç›¸å…³ç»“æœæ’åçš„å€’æ•°",
                "interpretation": get_mrr_interpretation(result.get("mrr", 0.0))
            },
            "ndcg": {
                "value": result.get("ndcg", 0.0),
                "description": "å½’ä¸€åŒ–æŠ˜æŸç´¯ç§¯å¢ç›Š - è€ƒè™‘ç»“æœæ’åºè´¨é‡çš„æŒ‡æ ‡",
                "interpretation": get_score_interpretation(result.get("ndcg", 0.0))
            },
            "diversity": {
                "value": result.get("diversity", 0.0),
                "description": "ç»“æœå¤šæ ·æ€§åˆ†æ•° - åŸºäºæ–‡ä»¶ç±»å‹å’Œç›®å½•çš„å¤šæ ·æ€§",
                "interpretation": get_diversity_interpretation(result.get("diversity", 0.0))
            }
        }
    
    return formatted_result

def get_score_interpretation(score):
    """è·å–åˆ†æ•°è§£é‡Š"""
    if score >= 0.8:
        return {"level": "ä¼˜ç§€", "color": "ğŸŸ¢", "description": "è¡¨ç°å¾ˆå¥½"}
    elif score >= 0.6:
        return {"level": "è‰¯å¥½", "color": "ğŸŸ¡", "description": "è¡¨ç°ä¸é”™ï¼Œæœ‰æ”¹è¿›ç©ºé—´"}
    elif score >= 0.4:
        return {"level": "ä¸€èˆ¬", "color": "ğŸŸ ", "description": "è¡¨ç°ä¸€èˆ¬ï¼Œéœ€è¦ä¼˜åŒ–"}
    elif score >= 0.2:
        return {"level": "è¾ƒå·®", "color": "ğŸ”´", "description": "è¡¨ç°è¾ƒå·®ï¼Œæ€¥éœ€æ”¹è¿›"}
    else:
        return {"level": "å¾ˆå·®", "color": "âš«", "description": "è¡¨ç°å¾ˆå·®ï¼Œéœ€è¦é‡æ–°è®¾è®¡"}

def get_total_score_interpretation(total_score):
    """è·å–æ€»åˆ†çš„ç‰¹æ®Šè§£é‡Š"""
    interpretation = get_score_interpretation(total_score)
    
    if total_score >= 0.8:
        interpretation["advice"] = "ç»¼åˆè¡¨ç°ä¼˜ç§€ï¼Œæ£€ç´¢ç³»ç»Ÿå¯ä»¥æŠ•å…¥ä½¿ç”¨"
        interpretation["level_detail"] = "åœ¨ç›¸å…³æ€§ã€å…¨é¢æ€§å’Œå¯ç”¨æ€§æ–¹é¢éƒ½è¡¨ç°å‡ºè‰²"
    elif total_score >= 0.6:
        interpretation["advice"] = "ç»¼åˆè¡¨ç°è‰¯å¥½ï¼Œå»ºè®®é’ˆå¯¹è–„å¼±ç¯èŠ‚è¿›ä¸€æ­¥ä¼˜åŒ–"
        interpretation["level_detail"] = "æ•´ä½“è¡¨ç°ä¸é”™ï¼Œä½†ä»æœ‰æ”¹è¿›ç©ºé—´"
    elif total_score >= 0.4:
        interpretation["advice"] = "ç»¼åˆè¡¨ç°ä¸€èˆ¬ï¼Œéœ€è¦é‡ç‚¹æ”¹è¿›ç›¸å…³æ€§å’Œå…¨é¢æ€§"
        interpretation["level_detail"] = "éœ€è¦åœ¨å¤šä¸ªç»´åº¦ä¸Šè¿›è¡Œä¼˜åŒ–"
    elif total_score >= 0.2:
        interpretation["advice"] = "ç»¼åˆè¡¨ç°è¾ƒå·®ï¼Œå»ºè®®é‡æ–°å®¡è§†æ£€ç´¢ç­–ç•¥"
        interpretation["level_detail"] = "åœ¨ç›¸å…³æ€§ã€å…¨é¢æ€§æˆ–å¯ç”¨æ€§æ–¹é¢å­˜åœ¨æ˜æ˜¾é—®é¢˜"
    else:
        interpretation["advice"] = "ç»¼åˆè¡¨ç°å¾ˆå·®ï¼Œéœ€è¦é‡æ–°è®¾è®¡æ£€ç´¢ç³»ç»Ÿ"
        interpretation["level_detail"] = "å„é¡¹æŒ‡æ ‡éƒ½éœ€è¦å¤§å¹…æ”¹è¿›"
    
    return interpretation

def get_mrr_interpretation(mrr):
    """è·å–MRRçš„è§£é‡Š"""
    if mrr >= 0.8:
        return {"level": "ä¼˜ç§€", "color": "ğŸŸ¢", "description": "ç¬¬ä¸€ä¸ªç»“æœé€šå¸¸å°±æ˜¯ç›¸å…³çš„"}
    elif mrr >= 0.5:
        return {"level": "è‰¯å¥½", "color": "ğŸŸ¡", "description": "ç›¸å…³ç»“æœé€šå¸¸åœ¨å‰å‡ ä½"}
    elif mrr >= 0.3:
        return {"level": "ä¸€èˆ¬", "color": "ğŸŸ ", "description": "ç›¸å…³ç»“æœæ’åè¾ƒé å"}
    else:
        return {"level": "è¾ƒå·®", "color": "ğŸ”´", "description": "å¾ˆéš¾æ‰¾åˆ°ç›¸å…³ç»“æœæˆ–æ’åå¾ˆé å"}

def get_diversity_interpretation(diversity):
    """è·å–å¤šæ ·æ€§çš„è§£é‡Š"""
    if diversity >= 0.7:
        return {"level": "é«˜", "description": "ç»“æœæ¶µç›–å¤šç§æ–‡ä»¶ç±»å‹å’Œç›®å½•"}
    elif diversity >= 0.4:
        return {"level": "ä¸­ç­‰", "description": "ç»“æœæœ‰ä¸€å®šå¤šæ ·æ€§"}
    else:
        return {"level": "ä½", "description": "ç»“æœæ¯”è¾ƒå•ä¸€ï¼Œå¯èƒ½è¿‡äºé›†ä¸­"}

def get_retrieval_score_interpretation(avg_score):
    """è·å–æ£€ç´¢åˆ†æ•°çš„è§£é‡Š"""
    if avg_score >= 0.7:
        return {"level": "é«˜", "description": "æ£€ç´¢ç®—æ³•åŒ¹é…åº¦å¾ˆé«˜"}
    elif avg_score >= 0.5:
        return {"level": "ä¸­ç­‰", "description": "æ£€ç´¢ç®—æ³•åŒ¹é…åº¦ä¸€èˆ¬"}
    else:
        return {"level": "ä½", "description": "æ£€ç´¢ç®—æ³•åŒ¹é…åº¦è¾ƒä½ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´"}

def print_formatted_result(result, show_details=True):
    """
    æ‰“å°æ ¼å¼åŒ–çš„è¯„ä¼°ç»“æœ
    
    Args:
        result: è¯„ä¼°ç»“æœ
        show_details: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    """
    if not result.get("success", False):
        print(f"æŸ¥è¯¢å¤±è´¥: {result.get('query', 'Unknown')} - {result.get('error', 'Unknown error')}")
        return
    
    print(f"\næŸ¥è¯¢: {result.get('query', 'Unknown')}")
    print(f"ç±»åˆ«: {result.get('category', 'Unknown')}")
    
    # æ˜¾ç¤ºæ–°è¯„ä¼°æ¡†æ¶æŒ‡æ ‡
    if "new_framework_metrics" in result:
        framework = result["new_framework_metrics"]
        
        print("\næ–°è¯„ä¼°æ¡†æ¶æŒ‡æ ‡:")
        
        # æ€»åˆ†
        total_info = framework["total_score"]
        total_interp = total_info["interpretation"]
        print(f"  {total_interp['color']} ç»¼åˆè¯„åˆ†: {total_info['value']:.3f} ({total_interp['level']})")
        if show_details:
            print(f"     â””â”€ {total_info['description']}")
            print(f"     {total_interp.get('advice', '')}")
        
        # ä¸‰ä¸ªç»´åº¦
        dimensions = ["relevance", "completeness", "usability"]
        dimension_names = {"relevance": "ç›¸å…³æ€§", "completeness": "å…¨é¢æ€§", "usability": "å¯ç”¨æ€§"}
        
        for dim in dimensions:
            if dim in framework:
                dim_info = framework[dim]
                dim_interp = dim_info["interpretation"]
                print(f"  {dim_interp['color']} {dimension_names[dim]}: {dim_info['value']:.3f} ({dim_interp['level']})")
                if show_details:
                    print(f"     â””â”€ {dim_info['description']}")
    
    if show_details and "path_matching_explanation" in result:
        path_exp = result["path_matching_explanation"]
        print(f"\nè·¯å¾„åŒ¹é…åˆ†æ:")
        print(f"  æ€»åˆ†: {path_exp['total_score']['value']:.3f}")
        
        match_details = path_exp["match_details"]
        print(f"  ç²¾ç¡®åŒ¹é…: {match_details['exact_matches']['count']} ä¸ª")
        print(f"  éƒ¨åˆ†åŒ¹é…: {match_details['partial_matches']['count']} ä¸ª")
        print(f"  æ‰©å±•ååŒ¹é…: {match_details['extension_matches']['count']} ä¸ª")
    
    if show_details and "advanced_metrics_explanation" in result:
        adv_metrics = result["advanced_metrics_explanation"]
        print(f"\né«˜çº§æŒ‡æ ‡:")
        for metric_name, metric_info in adv_metrics.items():
            print(f"  â€¢ {metric_name.upper()}: {metric_info['value']:.3f} - {metric_info['description']}")

def format_summary_with_explanations(summary_metrics):
    """ä¸ºæ±‡æ€»æŒ‡æ ‡æ·»åŠ è§£é‡Š"""
    formatted_summary = summary_metrics.copy()
    
    if "new_framework_performance" in summary_metrics:
        new_framework = summary_metrics["new_framework_performance"]
        formatted_summary["new_framework_explanation"] = {
            "description": "æ–°è¯„ä¼°æ¡†æ¶çš„å¹³å‡è¡¨ç°",
            "metrics": {
                "avg_total_score": {
                    "value": new_framework.get("avg_total_score", 0.0),
                    "interpretation": get_total_score_interpretation(new_framework.get("avg_total_score", 0.0)),
                    "importance": "æœ€é‡è¦çš„ç»¼åˆæŒ‡æ ‡"
                },
                "avg_relevance": {
                    "value": new_framework.get("avg_relevance", 0.0),
                    "interpretation": get_score_interpretation(new_framework.get("avg_relevance", 0.0)),
                    "importance": "è¡¡é‡ç»“æœç›¸å…³æ€§"
                },
                "avg_completeness": {
                    "value": new_framework.get("avg_completeness", 0.0),
                    "interpretation": get_score_interpretation(new_framework.get("avg_completeness", 0.0)),
                    "importance": "è¡¡é‡ç»“æœå…¨é¢æ€§"
                },
                "avg_usability": {
                    "value": new_framework.get("avg_usability", 0.0),
                    "interpretation": get_score_interpretation(new_framework.get("avg_usability", 0.0)),
                    "importance": "è¡¡é‡ç»“æœå¯ç”¨æ€§"
                }
            }
        }
    
    return formatted_summary 