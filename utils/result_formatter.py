# -*- coding: utf-8 -*-
"""
è¯„ä¼°ç»“æœæ ¼å¼åŒ–å·¥å…·
ç”¨äºä¸ºè¯„ä¼°ç»“æœæ·»åŠ è¯¦ç»†çš„æ³¨é‡Šå’Œè¯´æ˜
"""

def format_evaluation_result(result):
    """
    æ ¼å¼åŒ–å•ä¸ªè¯„ä¼°ç»“æœï¼Œæ·»åŠ è¯¦ç»†æ³¨é‡Š
    
    Args:
        result: è¯„ä¼°ç»“æœå­—å…¸
        
    Returns:
        dict: åŒ…å«æ³¨é‡Šçš„æ ¼å¼åŒ–ç»“æœ
    """
    if not result.get("success", False):
        return result
    
    # æ·»åŠ æŒ‡æ ‡è§£é‡Š
    formatted_result = result.copy()
    
    # æ–°è¯„ä¼°æ¡†æ¶æŒ‡æ ‡æ³¨é‡Š
    formatted_result["framework_explanation"] = {
        "total_score": {
            "value": result.get("total_score", 0.0),
            "description": "ç»¼åˆè¯„åˆ† - ç›¸å…³æ€§Ã—0.5 + å…¨é¢æ€§Ã—0.3 + å¯ç”¨æ€§Ã—0.2",
            "formula": "ç›¸å…³æ€§Ã—0.5 + å…¨é¢æ€§Ã—0.3 + å¯ç”¨æ€§Ã—0.2",
            "range": "0.0-1.0 (è¶Šé«˜è¶Šå¥½)",
            "interpretation": get_total_score_interpretation(result.get("total_score", 0.0))
        },
        "relevance": {
            "value": result.get("relevance", 0.0),
            "description": "ç›¸å…³æ€§ (50%) - å‰kä¸ªç»“æœä¸­ç›¸å…³æ•°/k",
            "formula": "å‰kä¸ªç»“æœä¸­ç›¸å…³æ•° / k",
            "range": "0.0-1.0 (è¶Šé«˜è¶Šå¥½)",
            "interpretation": get_score_interpretation(result.get("relevance", 0.0))
        },
        "completeness": {
            "value": result.get("completeness", 0.0),
            "description": "å…¨é¢æ€§ (30%) - å‰kä¸ªç»“æœä¸­çš„ç›¸å…³æ•°/æ€»ç›¸å…³æ•°",
            "formula": "å‰kä¸ªç»“æœä¸­çš„ç›¸å…³æ•° / æ€»ç›¸å…³æ•°",
            "range": "0.0-1.0 (è¶Šé«˜è¶Šå¥½)",
            "interpretation": get_score_interpretation(result.get("completeness", 0.0))
        },
        "usability": {
            "value": result.get("usability", 0.0),
            "description": "å¯ç”¨æ€§ (20%) - å¹³å‡å€’æ•°æ’å (MRR)",
            "formula": "ç¬¬ä¸€ä¸ªç›¸å…³ç»“æœæ’åçš„å€’æ•°",
            "range": "0.0-1.0 (è¶Šé«˜è¶Šå¥½)",
            "interpretation": get_mrr_interpretation(result.get("usability", 0.0))
        }
    }
    
    # ä¼ ç»ŸæŒ‡æ ‡æ³¨é‡Š (ä¿ç•™ç”¨äºå¯¹æ¯”)
    formatted_result["traditional_metrics_explanation"] = {
        "precision": {
            "value": result.get("precision", 0.0),
            "description": "ä¼ ç»Ÿç²¾ç¡®ç‡ - æ£€ç´¢åˆ°çš„ç»“æœä¸­ç›¸å…³ç»“æœçš„æ¯”ä¾‹",
            "formula": "ç›¸å…³ä¸”æ£€ç´¢åˆ°çš„ç»“æœæ•° / æ€»æ£€ç´¢åˆ°çš„ç»“æœæ•°",
            "range": "0.0-1.0 (è¶Šé«˜è¶Šå¥½)",
            "interpretation": get_score_interpretation(result.get("precision", 0.0))
        },
        "recall": {
            "value": result.get("recall", 0.0),
            "description": "ä¼ ç»Ÿå¬å›ç‡ - æ‰€æœ‰ç›¸å…³ç»“æœä¸­è¢«æ£€ç´¢åˆ°çš„æ¯”ä¾‹",
            "formula": "ç›¸å…³ä¸”æ£€ç´¢åˆ°çš„ç»“æœæ•° / æ€»ç›¸å…³ç»“æœæ•°",
            "range": "0.0-1.0 (è¶Šé«˜è¶Šå¥½)",
            "interpretation": get_score_interpretation(result.get("recall", 0.0))
        },
        "f1_score": {
            "value": result.get("f1_score", 0.0),
            "description": "ä¼ ç»ŸF1åˆ†æ•° - ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡æ•°",
            "formula": "2 Ã— (ç²¾ç¡®ç‡ Ã— å¬å›ç‡) / (ç²¾ç¡®ç‡ + å¬å›ç‡)",
            "range": "0.0-1.0 (è¶Šé«˜è¶Šå¥½)",
            "interpretation": get_f1_interpretation(result.get("f1_score", 0.0))
        }
    }
    
    # è·¯å¾„åŒ¹é…æ³¨é‡Š
    if "path_matching" in result:
        path_matching = result["path_matching"]
        formatted_result["path_matching_explanation"] = {
            "total_score": {
                "value": path_matching.get("total_score", 0.0),
                "description": "è·¯å¾„åŒ¹é…æ€»åˆ† - ç»¼åˆè€ƒè™‘ç²¾ç¡®ã€éƒ¨åˆ†å’Œæ‰©å±•ååŒ¹é…",
                "formula": "(ç²¾ç¡®åŒ¹é…Ã—1.0 + éƒ¨åˆ†åŒ¹é…Ã—0.7 + æ‰©å±•ååŒ¹é…Ã—0.3) / æœŸæœ›ç»“æœæ•°",
                "interpretation": get_score_interpretation(path_matching.get("total_score", 0.0))
            },
            "match_details": {
                "exact_matches": {
                    "count": path_matching.get("exact_matches", 0),
                    "description": "æ–‡ä»¶è·¯å¾„å®Œå…¨ç›¸åŒçš„åŒ¹é…æ•°",
                    "weight": "1.0 (æœ€é«˜æƒé‡)"
                },
                "partial_matches": {
                    "count": path_matching.get("partial_matches", 0),
                    "description": "æ–‡ä»¶åç›¸åŒæˆ–è·¯å¾„æœ‰é‡å çš„åŒ¹é…æ•°",
                    "weight": "0.7"
                },
                "extension_matches": {
                    "count": path_matching.get("extension_matches", 0),
                    "description": "æ–‡ä»¶æ‰©å±•åç›¸åŒçš„åŒ¹é…æ•°",
                    "weight": "0.3"
                }
            }
        }
    
    # Top-Kå‡†ç¡®ç‡æ³¨é‡Š
    if "top_k_accuracy" in result:
        top_k = result["top_k_accuracy"]
        formatted_result["top_k_explanation"] = {}
        for k, accuracy in top_k.items():
            formatted_result["top_k_explanation"][f"top_{k}"] = {
                "value": accuracy,
                "description": f"å‰{k}ä¸ªç»“æœä¸­ç›¸å…³ç»“æœçš„æ¯”ä¾‹",
                "interpretation": get_score_interpretation(accuracy)
            }
    
    # åˆ†æ•°åˆ†ææ³¨é‡Š
    if "score_analysis" in result:
        score_analysis = result["score_analysis"]
        formatted_result["score_analysis_explanation"] = {
            "avg_score": {
                "value": score_analysis.get("avg_score", 0.0),
                "description": "æ£€ç´¢ç»“æœçš„å¹³å‡åˆ†æ•°",
                "interpretation": get_retrieval_score_interpretation(score_analysis.get("avg_score", 0.0))
            },
            "max_score": {
                "value": score_analysis.get("max_score", 0.0),
                "description": "æ£€ç´¢ç»“æœçš„æœ€é«˜åˆ†æ•°",
                "interpretation": "åæ˜ æœ€ç›¸å…³ç»“æœçš„åŒ¹é…ç¨‹åº¦"
            },
            "score_gap": {
                "value": score_analysis.get("score_gap", 0.0),
                "description": "æœ€é«˜åˆ†å’Œæœ€ä½åˆ†çš„å·®è·",
                "interpretation": "å·®è·å¤§è¯´æ˜ç»“æœè´¨é‡å·®å¼‚æ˜æ˜¾"
            }
        }
    
    # é«˜çº§æŒ‡æ ‡æ³¨é‡Š
    if "mrr" in result:
        formatted_result["advanced_metrics_explanation"] = {
            "mrr": {
                "value": result.get("mrr", 0.0),
                "description": "å¹³å‡å€’æ•°æ’å - ç¬¬ä¸€ä¸ªç›¸å…³ç»“æœæ’åçš„å€’æ•°",
                "interpretation": get_mrr_interpretation(result.get("mrr", 0.0))
            },
            "ndcg": {
                "value": result.get("ndcg", 0.0),
                "description": "å½’ä¸€åŒ–æŠ˜æŸç´¯ç§¯å¢ç›Š - è€ƒè™‘ç»“æœæ’åºè´¨é‡çš„æŒ‡æ ‡",
                "interpretation": get_score_interpretation(result.get("ndcg", 0.0))
            },
            "diversity": {
                "value": result.get("diversity", 0.0),
                "description": "ç»“æœå¤šæ ·æ€§åˆ†æ•° - åŸºäºæ–‡ä»¶ç±»å‹å’Œç›®å½•çš„å¤šæ ·æ€§",
                "interpretation": get_diversity_interpretation(result.get("diversity", 0.0))
            }
        }
    
    return formatted_result

def get_score_interpretation(score):
    """è·å–åˆ†æ•°è§£é‡Š"""
    if score >= 0.8:
        return {"level": "ä¼˜ç§€", "color": "ğŸŸ¢", "description": "è¡¨ç°å¾ˆå¥½"}
    elif score >= 0.6:
        return {"level": "è‰¯å¥½", "color": "ğŸŸ¡", "description": "è¡¨ç°ä¸é”™ï¼Œæœ‰æ”¹è¿›ç©ºé—´"}
    elif score >= 0.4:
        return {"level": "ä¸€èˆ¬", "color": "ğŸŸ ", "description": "è¡¨ç°ä¸€èˆ¬ï¼Œéœ€è¦ä¼˜åŒ–"}
    elif score >= 0.2:
        return {"level": "è¾ƒå·®", "color": "ğŸ”´", "description": "è¡¨ç°è¾ƒå·®ï¼Œæ€¥éœ€æ”¹è¿›"}
    else:
        return {"level": "å¾ˆå·®", "color": "âš«", "description": "è¡¨ç°å¾ˆå·®ï¼Œéœ€è¦é‡æ–°è®¾è®¡"}

def get_total_score_interpretation(total_score):
    """è·å–æ€»åˆ†çš„ç‰¹æ®Šè§£é‡Š"""
    interpretation = get_score_interpretation(total_score)
    
    if total_score >= 0.8:
        interpretation["advice"] = "ç»¼åˆè¡¨ç°ä¼˜ç§€ï¼Œæ£€ç´¢ç³»ç»Ÿå¯ä»¥æŠ•å…¥ä½¿ç”¨"
        interpretation["level_detail"] = "åœ¨ç›¸å…³æ€§ã€å…¨é¢æ€§å’Œå¯ç”¨æ€§æ–¹é¢éƒ½è¡¨ç°å‡ºè‰²"
    elif total_score >= 0.6:
        interpretation["advice"] = "ç»¼åˆè¡¨ç°è‰¯å¥½ï¼Œå»ºè®®é’ˆå¯¹è–„å¼±ç¯èŠ‚è¿›ä¸€æ­¥ä¼˜åŒ–"
        interpretation["level_detail"] = "æ•´ä½“è¡¨ç°ä¸é”™ï¼Œä½†ä»æœ‰æ”¹è¿›ç©ºé—´"
    elif total_score >= 0.4:
        interpretation["advice"] = "ç»¼åˆè¡¨ç°ä¸€èˆ¬ï¼Œéœ€è¦é‡ç‚¹æ”¹è¿›ç›¸å…³æ€§å’Œå…¨é¢æ€§"
        interpretation["level_detail"] = "éœ€è¦åœ¨å¤šä¸ªç»´åº¦ä¸Šè¿›è¡Œä¼˜åŒ–"
    elif total_score >= 0.2:
        interpretation["advice"] = "ç»¼åˆè¡¨ç°è¾ƒå·®ï¼Œå»ºè®®é‡æ–°å®¡è§†æ£€ç´¢ç­–ç•¥"
        interpretation["level_detail"] = "åœ¨ç›¸å…³æ€§ã€å…¨é¢æ€§æˆ–å¯ç”¨æ€§æ–¹é¢å­˜åœ¨æ˜æ˜¾é—®é¢˜"
    else:
        interpretation["advice"] = "ç»¼åˆè¡¨ç°å¾ˆå·®ï¼Œéœ€è¦é‡æ–°è®¾è®¡æ£€ç´¢ç³»ç»Ÿ"
        interpretation["level_detail"] = "å„é¡¹æŒ‡æ ‡éƒ½éœ€è¦å¤§å¹…æ”¹è¿›"
    
    return interpretation

def get_f1_interpretation(f1_score):
    """è·å–F1åˆ†æ•°çš„ç‰¹æ®Šè§£é‡Š"""
    interpretation = get_score_interpretation(f1_score)
    
    if f1_score >= 0.7:
        interpretation["advice"] = "ä¼ ç»ŸF1æŒ‡æ ‡ä¼˜ç§€ï¼Œå¯ä»¥æŠ•å…¥ä½¿ç”¨"
    elif f1_score >= 0.5:
        interpretation["advice"] = "ä¼ ç»ŸF1æŒ‡æ ‡å¯ä»¥æ¥å—ï¼Œå»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–"
    elif f1_score >= 0.3:
        interpretation["advice"] = "ä¼ ç»ŸF1æŒ‡æ ‡éœ€è¦æ”¹è¿›ï¼Œæ£€æŸ¥ç®—æ³•å’Œæ•°æ®"
    else:
        interpretation["advice"] = "ä¼ ç»ŸF1æŒ‡æ ‡å¾ˆå·®ï¼Œéœ€è¦é‡æ–°è®¾è®¡æ£€ç´¢ç­–ç•¥"
    
    return interpretation

def get_mrr_interpretation(mrr):
    """è·å–MRRçš„è§£é‡Š"""
    if mrr >= 0.8:
        return {"level": "ä¼˜ç§€", "color": "ğŸŸ¢", "description": "ç¬¬ä¸€ä¸ªç»“æœé€šå¸¸å°±æ˜¯ç›¸å…³çš„"}
    elif mrr >= 0.5:
        return {"level": "è‰¯å¥½", "color": "ğŸŸ¡", "description": "ç›¸å…³ç»“æœé€šå¸¸åœ¨å‰å‡ ä½"}
    elif mrr >= 0.3:
        return {"level": "ä¸€èˆ¬", "color": "ğŸŸ ", "description": "ç›¸å…³ç»“æœæ’åè¾ƒé å"}
    else:
        return {"level": "è¾ƒå·®", "color": "ğŸ”´", "description": "å¾ˆéš¾æ‰¾åˆ°ç›¸å…³ç»“æœæˆ–æ’åå¾ˆé å"}

def get_diversity_interpretation(diversity):
    """è·å–å¤šæ ·æ€§çš„è§£é‡Š"""
    if diversity >= 0.7:
        return {"level": "é«˜", "description": "ç»“æœæ¶µç›–å¤šç§æ–‡ä»¶ç±»å‹å’Œç›®å½•"}
    elif diversity >= 0.4:
        return {"level": "ä¸­ç­‰", "description": "ç»“æœæœ‰ä¸€å®šå¤šæ ·æ€§"}
    else:
        return {"level": "ä½", "description": "ç»“æœæ¯”è¾ƒå•ä¸€ï¼Œå¯èƒ½è¿‡äºé›†ä¸­"}

def get_retrieval_score_interpretation(avg_score):
    """è·å–æ£€ç´¢åˆ†æ•°çš„è§£é‡Š"""
    if avg_score >= 0.7:
        return {"level": "é«˜", "description": "æ£€ç´¢ç®—æ³•åŒ¹é…åº¦å¾ˆé«˜"}
    elif avg_score >= 0.5:
        return {"level": "ä¸­ç­‰", "description": "æ£€ç´¢ç®—æ³•åŒ¹é…åº¦ä¸€èˆ¬"}
    else:
        return {"level": "ä½", "description": "æ£€ç´¢ç®—æ³•åŒ¹é…åº¦è¾ƒä½ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´"}

def print_formatted_result(result, show_details=True):
    """
    æ‰“å°æ ¼å¼åŒ–çš„è¯„ä¼°ç»“æœ
    
    Args:
        result: è¯„ä¼°ç»“æœ
        show_details: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    """
    if not result.get("success", False):
        print(f"âŒ æŸ¥è¯¢å¤±è´¥: {result.get('query', 'Unknown')} - {result.get('error', 'Unknown error')}")
        return
    
    print(f"\nğŸ“Š æŸ¥è¯¢: {result.get('query', 'Unknown')}")
    print(f"ğŸ·ï¸  ç±»åˆ«: {result.get('category', 'Unknown')}")
    
    # æ˜¾ç¤ºæ–°è¯„ä¼°æ¡†æ¶æŒ‡æ ‡
    if "framework_explanation" in result:
        framework = result["framework_explanation"]
        
        print("\nğŸ¯ æ–°è¯„ä¼°æ¡†æ¶æŒ‡æ ‡:")
        
        # æ€»åˆ†
        total_info = framework["total_score"]
        total_interp = total_info["interpretation"]
        print(f"  {total_interp['color']} ç»¼åˆè¯„åˆ†: {total_info['value']:.3f} ({total_interp['level']})")
        if show_details:
            print(f"     â””â”€ {total_info['description']}")
            print(f"     ğŸ’¡ {total_interp.get('advice', '')}")
        
        # ä¸‰ä¸ªç»´åº¦
        dimensions = ["relevance", "completeness", "usability"]
        dimension_names = {"relevance": "ç›¸å…³æ€§", "completeness": "å…¨é¢æ€§", "usability": "å¯ç”¨æ€§"}
        
        for dim in dimensions:
            if dim in framework:
                dim_info = framework[dim]
                dim_interp = dim_info["interpretation"]
                print(f"  {dim_interp['color']} {dimension_names[dim]}: {dim_info['value']:.3f} ({dim_interp['level']})")
                if show_details:
                    print(f"     â””â”€ {dim_info['description']}")
    
    # æ˜¾ç¤ºä¼ ç»ŸæŒ‡æ ‡ (æŠ˜å æ˜¾ç¤º)
    if show_details and "traditional_metrics_explanation" in result:
        traditional = result["traditional_metrics_explanation"]
        
        print("\nğŸ“ˆ ä¼ ç»ŸæŒ‡æ ‡ (å¯¹æ¯”å‚è€ƒ):")
        for metric_name, metric_info in traditional.items():
            value = metric_info["value"]
            interp = metric_info["interpretation"]
            print(f"  {interp['color']} {metric_name.replace('_', ' ').title()}: {value:.3f} ({interp['level']})")
            print(f"     â””â”€ {metric_info['description']}")
    
    if show_details and "path_matching_explanation" in result:
        path_exp = result["path_matching_explanation"]
        print(f"\nğŸ” è·¯å¾„åŒ¹é…åˆ†æ:")
        print(f"  ğŸ“ˆ æ€»åˆ†: {path_exp['total_score']['value']:.3f}")
        
        match_details = path_exp["match_details"]
        print(f"  âœ… ç²¾ç¡®åŒ¹é…: {match_details['exact_matches']['count']} ä¸ª")
        print(f"  ğŸ¯ éƒ¨åˆ†åŒ¹é…: {match_details['partial_matches']['count']} ä¸ª")
        print(f"  ğŸ“„ æ‰©å±•ååŒ¹é…: {match_details['extension_matches']['count']} ä¸ª")
    
    if show_details and "advanced_metrics_explanation" in result:
        adv_metrics = result["advanced_metrics_explanation"]
        print(f"\nğŸ“ˆ é«˜çº§æŒ‡æ ‡:")
        for metric_name, metric_info in adv_metrics.items():
            print(f"  â€¢ {metric_name.upper()}: {metric_info['value']:.3f} - {metric_info['description']}")

def format_summary_with_explanations(summary_metrics):
    """ä¸ºæ±‡æ€»æŒ‡æ ‡æ·»åŠ è§£é‡Š"""
    formatted_summary = summary_metrics.copy()
    
    if "overall_performance" in summary_metrics:
        overall = summary_metrics["overall_performance"]
        formatted_summary["overall_performance_explanation"] = {
            "description": "æ‰€æœ‰æµ‹è¯•æ¡ˆä¾‹çš„å¹³å‡è¡¨ç°",
            "metrics": {
                "avg_f1_score": {
                    "value": overall.get("avg_f1_score", 0.0),
                    "interpretation": get_f1_interpretation(overall.get("avg_f1_score", 0.0)),
                    "importance": "æœ€é‡è¦çš„ç»¼åˆæŒ‡æ ‡"
                },
                "avg_precision": {
                    "value": overall.get("avg_precision", 0.0),
                    "interpretation": get_score_interpretation(overall.get("avg_precision", 0.0)),
                    "importance": "è¡¡é‡ç»“æœå‡†ç¡®æ€§"
                },
                "avg_recall": {
                    "value": overall.get("avg_recall", 0.0),
                    "interpretation": get_score_interpretation(overall.get("avg_recall", 0.0)),
                    "importance": "è¡¡é‡ç»“æœå®Œæ•´æ€§"
                }
            }
        }
    
    return formatted_summary 