# -*- coding: utf-8 -*-
"""
ä»£ç æ£€ç´¢è¯„ä¼°ç³»ç»Ÿä¸»è¿è¡Œè„šæœ¬
"""

import os
import sys
import argparse
import logging
import json
from datetime import datetime
from pathlib import Path

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(__file__))

from config import (
    API_CONFIG, EVALUATION_CONFIG, CATEGORY_CONFIG, 
    PERFORMANCE_CONFIG, PATH_CONFIG, LOGGING_CONFIG,
    validate_config
)
from evaluator import CodeSearchEvaluator
from utils.result_formatter import format_evaluation_result, print_formatted_result, format_summary_with_explanations

def setup_logging(debug=False):
    """è®¾ç½®æ—¥å¿—é…ç½®"""
    level = logging.DEBUG if debug else getattr(logging, LOGGING_CONFIG["level"])
    
    # åˆ›å»ºæ ¼å¼åŒ–å™¨
    formatter = logging.Formatter(LOGGING_CONFIG["format"])
    
    # è®¾ç½®æ ¹æ—¥å¿—å™¨
    logger = logging.getLogger()
    logger.setLevel(level)
    
    # æ¸…é™¤ç°æœ‰å¤„ç†å™¨
    logger.handlers.clear()
    
    # æ§åˆ¶å°å¤„ç†å™¨
    if LOGGING_CONFIG["console_output"]:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        console_handler.setFormatter(formatter)
        logger.addHandler(console_handler)
    
    # æ–‡ä»¶å¤„ç†å™¨
    if LOGGING_CONFIG["file"]:
        file_handler = logging.FileHandler(
            LOGGING_CONFIG["file"], 
            encoding='utf-8'
        )
        file_handler.setLevel(level)
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)

def create_directories():
    """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
    directories = [
        PATH_CONFIG["results_dir"],
        PATH_CONFIG["reports_dir"],
        PATH_CONFIG["history_dir"]
    ]
    
    for directory in directories:
        Path(directory).mkdir(parents=True, exist_ok=True)

def run_evaluation(args):
    """è¿è¡Œè¯„ä¼°"""
    logger = logging.getLogger(__name__)
    
    try:
        # éªŒè¯é…ç½®
        validate_config()
        logger.info("é…ç½®éªŒè¯é€šè¿‡")
        
        # åˆ›å»ºå¿…è¦ç›®å½•
        create_directories()
        
        # æ„å»ºå®Œæ•´é…ç½®
        config = {
            "api": API_CONFIG,
            "evaluation": EVALUATION_CONFIG,
            "categories": CATEGORY_CONFIG,
            "performance": PERFORMANCE_CONFIG,
            "paths": PATH_CONFIG
        }
        
        # åˆ›å»ºè¯„ä¼°å™¨
        logger.info("åˆå§‹åŒ–ä»£ç æ£€ç´¢è¯„ä¼°å™¨...")
        evaluator = CodeSearchEvaluator(config)
        
        # æµ‹è¯•APIè¿æ¥
        if not evaluator.api_client.test_connection():
            logger.error("APIè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥æœåŠ¡çŠ¶æ€")
            return False
        
        logger.info("APIè¿æ¥æ­£å¸¸")
        
        # åŠ è½½æµ‹è¯•æ•°æ®é›†
        dataset_path = args.dataset or PATH_CONFIG["test_dataset"]
        logger.info(f"åŠ è½½æµ‹è¯•æ•°æ®é›†: {dataset_path}")
        
        if not os.path.exists(dataset_path):
            logger.error(f"æµ‹è¯•æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {dataset_path}")
            return False
        
        dataset = evaluator.load_test_dataset(dataset_path)
        
        # å¦‚æœæŒ‡å®šäº†æµ‹è¯•æ¡ˆä¾‹æ•°é‡é™åˆ¶
        if args.limit and args.limit > 0:
            original_count = len(dataset["test_cases"])
            dataset["test_cases"] = dataset["test_cases"][:args.limit]
            logger.info(f"é™åˆ¶æµ‹è¯•æ¡ˆä¾‹æ•°é‡: {original_count} -> {len(dataset['test_cases'])}")
        
        # å¦‚æœæŒ‡å®šäº†ç±»åˆ«è¿‡æ»¤
        if args.category:
            original_count = len(dataset["test_cases"])
            dataset["test_cases"] = [
                tc for tc in dataset["test_cases"] 
                if tc.get("category") == args.category
            ]
            logger.info(f"æŒ‰ç±»åˆ« '{args.category}' è¿‡æ»¤: {original_count} -> {len(dataset['test_cases'])}")
        
        if not dataset["test_cases"]:
            logger.error("æ²¡æœ‰æµ‹è¯•æ¡ˆä¾‹éœ€è¦è¯„ä¼°")
            return False
        
        # æ‰§è¡Œè¯„ä¼°
        logger.info("å¼€å§‹æ‰§è¡Œä»£ç æ£€ç´¢è¯„ä¼°...")
        results = evaluator.evaluate_dataset(dataset)
        
        # ä¿å­˜ç»“æœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜æœ€æ–°ç»“æœ
        latest_path = PATH_CONFIG["latest_result"]
        evaluator.save_results(results, latest_path)
        
        # ä¿å­˜å†å²ç»“æœ
        if args.save_history:
            history_path = os.path.join(
                PATH_CONFIG["history_dir"],
                f"evaluation_{timestamp}.json"
            )
            evaluator.save_results(results, history_path)
            logger.info(f"å†å²ç»“æœå·²ä¿å­˜: {history_path}")
        
        # ç”ŸæˆæŠ¥å‘Š
        if args.generate_report:
            generate_reports(results, timestamp)
        
        # æ˜¾ç¤ºç®€è¦ç»“æœ
        show_summary(results)
        
        # è¯†åˆ«é—®é¢˜æŸ¥è¯¢
        if args.show_problems:
            show_problematic_queries(evaluator)
        
        logger.info("è¯„ä¼°å®Œæˆ!")
        return True
        
    except Exception as e:
        logger.error(f"è¯„ä¼°è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        if args.debug:
            logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        return False

def generate_reports(results, timestamp):
    """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
    logger = logging.getLogger(__name__)
    
    try:
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        md_report_path = os.path.join(
            PATH_CONFIG["reports_dir"],
            f"evaluation_report_{timestamp}.md"
        )
        
        generate_markdown_report(results, md_report_path)
        logger.info(f"MarkdownæŠ¥å‘Šå·²ç”Ÿæˆ: {md_report_path}")
        
        # ç”Ÿæˆç®€è¦æŠ¥å‘Š
        summary_path = os.path.join(
            PATH_CONFIG["reports_dir"],
            f"summary_{timestamp}.txt"
        )
        
        generate_summary_report(results, summary_path)
        logger.info(f"ç®€è¦æŠ¥å‘Šå·²ç”Ÿæˆ: {summary_path}")
        
    except Exception as e:
        logger.error(f"ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")

def generate_markdown_report(results, output_path):
    """ç”ŸæˆMarkdownæ ¼å¼çš„è¯¦ç»†æŠ¥å‘Š"""
    summary = results["summary_metrics"]
    meta = results["meta"]
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write("# ä»£ç æ£€ç´¢è¯„ä¼°æŠ¥å‘Š\n\n")
        
        # åŸºæœ¬ä¿¡æ¯
        f.write("## è¯„ä¼°æ¦‚è§ˆ\n\n")
        f.write(f"- **è¯„ä¼°æ—¶é—´**: {meta['evaluation_time']}\n")
        f.write(f"- **æµ‹è¯•æ¡ˆä¾‹æ€»æ•°**: {meta['total_test_cases']}\n")
        f.write(f"- **æˆåŠŸè¯„ä¼°**: {meta['successful_evaluations']}\n")
        f.write(f"- **å¤±è´¥è¯„ä¼°**: {meta['failed_evaluations']}\n")
        f.write(f"- **æˆåŠŸç‡**: {summary['evaluation_statistics']['success_rate']:.1%}\n\n")
        
        # æ•´ä½“æ€§èƒ½
        f.write("## æ•´ä½“æ€§èƒ½æŒ‡æ ‡\n\n")
        overall = summary["overall_performance"]
        f.write(f"- **å¹³å‡ç²¾ç¡®ç‡**: {overall['avg_precision']:.3f}\n")
        f.write(f"- **å¹³å‡å¬å›ç‡**: {overall['avg_recall']:.3f}\n")
        f.write(f"- **å¹³å‡F1åˆ†æ•°**: {overall['avg_f1_score']:.3f}\n")
        f.write(f"- **å¹³å‡MRR**: {overall['avg_mrr']:.3f}\n")
        f.write(f"- **å¹³å‡NDCG**: {overall['avg_ndcg']:.3f}\n\n")
        
        # Top-Kæ€§èƒ½
        f.write("## Top-Kå‡†ç¡®ç‡\n\n")
        top_k = summary["top_k_performance"]
        for k, accuracy in top_k.items():
            f.write(f"- **{k.replace('_', '-').title()}**: {accuracy:.3f}\n")
        f.write("\n")
        
        # è·¯å¾„åŒ¹é…æ€§èƒ½
        f.write("## è·¯å¾„åŒ¹é…åˆ†æ\n\n")
        path_matching = summary["path_matching_performance"]
        f.write(f"- **å¹³å‡è·¯å¾„åŒ¹é…åˆ†æ•°**: {path_matching['avg_path_match_score']:.3f}\n")
        f.write(f"- **ç²¾ç¡®åŒ¹é…æ€»æ•°**: {path_matching['total_exact_matches']}\n")
        f.write(f"- **éƒ¨åˆ†åŒ¹é…æ€»æ•°**: {path_matching['total_partial_matches']}\n")
        f.write(f"- **ç²¾ç¡®åŒ¹é…ç‡**: {path_matching['exact_match_rate']:.3f}\n\n")
        
        # åˆ†ç±»åˆ«æ€§èƒ½
        if "category_metrics" in results:
            f.write("## åˆ†ç±»åˆ«æ€§èƒ½\n\n")
            for category, metrics in results["category_metrics"].items():
                f.write(f"### {metrics['name']}\n\n")
                f.write(f"- **æµ‹è¯•æ¡ˆä¾‹æ•°**: {metrics['count']}\n")
                f.write(f"- **æƒé‡**: {metrics['weight']}\n")
                f.write(f"- **å¹³å‡ç²¾ç¡®ç‡**: {metrics['avg_precision']:.3f}\n")
                f.write(f"- **å¹³å‡å¬å›ç‡**: {metrics['avg_recall']:.3f}\n")
                f.write(f"- **å¹³å‡F1åˆ†æ•°**: {metrics['avg_f1']:.3f}\n")
                f.write(f"- **åŠ æƒåˆ†æ•°**: {metrics['weighted_score']:.3f}\n\n")

def generate_summary_report(results, output_path):
    """ç”Ÿæˆç®€è¦æ–‡æœ¬æŠ¥å‘Š"""
    summary = results["summary_metrics"]
    meta = results["meta"]
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write("ä»£ç æ£€ç´¢è¯„ä¼°ç»“æœç®€è¦æŠ¥å‘Š\n")
        f.write("=" * 40 + "\n\n")
        
        f.write(f"è¯„ä¼°æ—¶é—´: {meta['evaluation_time']}\n")
        f.write(f"æµ‹è¯•æ¡ˆä¾‹: {meta['total_test_cases']} (æˆåŠŸ: {meta['successful_evaluations']})\n")
        f.write(f"æˆåŠŸç‡: {summary['evaluation_statistics']['success_rate']:.1%}\n\n")
        
        overall = summary["overall_performance"]
        f.write("æ•´ä½“æ€§èƒ½:\n")
        f.write(f"  ç²¾ç¡®ç‡: {overall['avg_precision']:.3f}\n")
        f.write(f"  å¬å›ç‡: {overall['avg_recall']:.3f}\n")
        f.write(f"  F1åˆ†æ•°: {overall['avg_f1_score']:.3f}\n")
        f.write(f"  MRR: {overall['avg_mrr']:.3f}\n")

def show_summary(results):
    """æ˜¾ç¤ºè¯„ä¼°ç»“æœæ‘˜è¦"""
    print("\n" + "="*60)
    print("ğŸ“Š ä»£ç æ£€ç´¢è¯„ä¼°ç»“æœæ‘˜è¦")
    print("="*60)
    
    meta = results["meta"]
    summary = results["summary_metrics"]
    
    print(f"â° è¯„ä¼°æ—¶é—´: {meta['evaluation_time']}")
    print(f"ğŸ“‹ æµ‹è¯•æ¡ˆä¾‹: {meta['total_test_cases']} (æˆåŠŸ: {meta['successful_evaluations']})")
    print(f"âœ… æˆåŠŸç‡: {summary['evaluation_statistics']['success_rate']:.1%}")
    
    # ä½¿ç”¨æ ¼å¼åŒ–å·¥å…·æ·»åŠ è¯¦ç»†è§£é‡Š
    formatted_summary = format_summary_with_explanations(summary)
    
    print("\nğŸ¯ æ–°è¯„ä¼°æ¡†æ¶æ•´ä½“è¡¨ç°:")
    if "new_framework_performance" in summary:
        new_framework = summary["new_framework_performance"]
        weights = new_framework.get("framework_weights", {})
        
        # æ€»åˆ†
        total_score = new_framework["avg_total_score"]
        total_interp = get_total_score_interpretation(total_score)
        print(f"  {total_interp['color']} å¹³å‡ç»¼åˆè¯„åˆ†: {total_score:.3f} ({total_interp['level']})")
        print(f"     ğŸ’¡ {total_interp.get('advice', '')}")
        
        # ä¸‰ä¸ªç»´åº¦
        dimensions = [
            ("avg_relevance", "å¹³å‡ç›¸å…³æ€§", weights.get("relevance", 0.5)),
            ("avg_completeness", "å¹³å‡å…¨é¢æ€§", weights.get("completeness", 0.3)),
            ("avg_usability", "å¹³å‡å¯ç”¨æ€§", weights.get("usability", 0.2))
        ]
        
        for metric_key, metric_name, weight in dimensions:
            value = new_framework.get(metric_key, 0.0)
            interp = get_score_interpretation(value)
            print(f"  {interp['color']} {metric_name}: {value:.3f} ({interp['level']}) - æƒé‡: {weight*100:.0f}%")
    
    # ä¼ ç»ŸæŒ‡æ ‡å¯¹æ¯”
    if "traditional_performance" in summary:
        traditional = summary["traditional_performance"]
        print(f"\nğŸ“ˆ ä¼ ç»ŸæŒ‡æ ‡å¯¹æ¯”:")
        print(f"  ğŸ“ å¹³å‡ç²¾ç¡®ç‡: {traditional['avg_precision']:.3f}")
        print(f"  ğŸª å¹³å‡å¬å›ç‡: {traditional['avg_recall']:.3f}")
        print(f"  âš–ï¸ å¹³å‡F1åˆ†æ•°: {traditional['avg_f1_score']:.3f}")

def get_total_score_interpretation(total_score):
    """è·å–æ€»åˆ†è§£é‡Š"""
    if total_score >= 0.8:
        return {"level": "ä¼˜ç§€", "color": "ğŸŸ¢", "advice": "ç»¼åˆè¡¨ç°ä¼˜ç§€ï¼Œæ£€ç´¢ç³»ç»Ÿå¯ä»¥æŠ•å…¥ä½¿ç”¨"}
    elif total_score >= 0.6:
        return {"level": "è‰¯å¥½", "color": "ğŸŸ¡", "advice": "ç»¼åˆè¡¨ç°è‰¯å¥½ï¼Œå»ºè®®é’ˆå¯¹è–„å¼±ç¯èŠ‚è¿›ä¸€æ­¥ä¼˜åŒ–"}
    elif total_score >= 0.4:
        return {"level": "ä¸€èˆ¬", "color": "ğŸŸ ", "advice": "ç»¼åˆè¡¨ç°ä¸€èˆ¬ï¼Œéœ€è¦é‡ç‚¹æ”¹è¿›ç›¸å…³æ€§å’Œå…¨é¢æ€§"}
    elif total_score >= 0.2:
        return {"level": "è¾ƒå·®", "color": "ğŸ”´", "advice": "ç»¼åˆè¡¨ç°è¾ƒå·®ï¼Œå»ºè®®é‡æ–°å®¡è§†æ£€ç´¢ç­–ç•¥"}
    else:
        return {"level": "å¾ˆå·®", "color": "âš«", "advice": "ç»¼åˆè¡¨ç°å¾ˆå·®ï¼Œéœ€è¦é‡æ–°è®¾è®¡æ£€ç´¢ç³»ç»Ÿ"}
    
    print("\nğŸ“ˆ Top-Kå‡†ç¡®ç‡:")
    top_k = summary["top_k_performance"]
    for k, accuracy in top_k.items():
        interp = get_score_interpretation(accuracy)
        print(f"  {interp['color']} {k.replace('_', '-').title()}: {accuracy:.3f} ({interp['level']})")
    
    # è·¯å¾„åŒ¹é…æ€§èƒ½
    if "path_matching_performance" in summary:
        path_perf = summary["path_matching_performance"]
        print(f"\nğŸ” è·¯å¾„åŒ¹é…æ€§èƒ½:")
        print(f"  ğŸ“ˆ å¹³å‡åŒ¹é…åˆ†æ•°: {path_perf['avg_path_match_score']:.3f}")
        print(f"  âœ… ç²¾ç¡®åŒ¹é…æ€»æ•°: {path_perf['total_exact_matches']}")
        print(f"  ğŸ¯ ç²¾ç¡®åŒ¹é…ç‡: {path_perf['exact_match_rate']:.3f}")
    
    # åˆ†æ•°åˆ†å¸ƒ
    if "score_distribution" in summary:
        score_dist = summary["score_distribution"]
        print(f"\nğŸ“Š æ£€ç´¢åˆ†æ•°åˆ†æ:")
        print(f"  ğŸ“ˆ å¹³å‡ç»“æœåˆ†æ•°: {score_dist['avg_result_score']:.3f}")
        print(f"  ğŸ” å¹³å‡æœ€é«˜åˆ†: {score_dist['avg_max_score']:.3f}")
        print(f"  ğŸ“ åˆ†æ•°ä¸€è‡´æ€§: {score_dist['score_consistency']:.3f}")
    
    print("\n" + "="*60)

def get_score_interpretation(score):
    """è·å–åˆ†æ•°è§£é‡Šï¼ˆä»result_formatterå¯¼å…¥ï¼‰"""
    if score >= 0.8:
        return {"level": "ä¼˜ç§€", "color": "ğŸŸ¢", "description": "è¡¨ç°å¾ˆå¥½"}
    elif score >= 0.6:
        return {"level": "è‰¯å¥½", "color": "ğŸŸ¡", "description": "è¡¨ç°ä¸é”™ï¼Œæœ‰æ”¹è¿›ç©ºé—´"}
    elif score >= 0.4:
        return {"level": "ä¸€èˆ¬", "color": "ğŸŸ ", "description": "è¡¨ç°ä¸€èˆ¬ï¼Œéœ€è¦ä¼˜åŒ–"}
    elif score >= 0.2:
        return {"level": "è¾ƒå·®", "color": "ğŸ”´", "description": "è¡¨ç°è¾ƒå·®ï¼Œæ€¥éœ€æ”¹è¿›"}
    else:
        return {"level": "å¾ˆå·®", "color": "âš«", "description": "è¡¨ç°å¾ˆå·®ï¼Œéœ€è¦é‡æ–°è®¾è®¡"}

def show_problematic_queries(evaluator):
    """æ˜¾ç¤ºé—®é¢˜æŸ¥è¯¢"""
    problematic = evaluator.get_problematic_queries(threshold=0.4)
    
    if not problematic:
        print("\nâœ… æ²¡æœ‰å‘ç°é—®é¢˜æŸ¥è¯¢ (æ€»åˆ† < 0.4)")
        return
    
    print(f"\nâš ï¸  å‘ç° {len(problematic)} ä¸ªé—®é¢˜æŸ¥è¯¢ (æ€»åˆ† < 0.4):")
    print("-" * 60)
    
    for i, query in enumerate(problematic[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ª
        print(f"{i+1}. {query['query']} (æ€»åˆ†: {query['total_score']:.3f})")
        print(f"   ç±»åˆ«: {query['category']}")
        print(f"   è¯¦æƒ…: ç›¸å…³æ€§={query['relevance']:.3f}, å…¨é¢æ€§={query['completeness']:.3f}, å¯ç”¨æ€§={query['usability']:.3f}")
        print(f"   é—®é¢˜: {', '.join(query['issues'])}")
        print()

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ä»£ç æ£€ç´¢è¯„ä¼°ç³»ç»Ÿ")
    
    parser.add_argument(
        "--dataset", "-d",
        type=str,
        help="æµ‹è¯•æ•°æ®é›†æ–‡ä»¶è·¯å¾„ (é»˜è®¤: test_dataset.json)"
    )
    
    parser.add_argument(
        "--limit", "-l",
        type=int,
        help="é™åˆ¶æµ‹è¯•æ¡ˆä¾‹æ•°é‡"
    )
    
    parser.add_argument(
        "--category", "-c",
        type=str,
        choices=["style", "function", "layout", "api"],
        help="æŒ‰ç±»åˆ«è¿‡æ»¤æµ‹è¯•æ¡ˆä¾‹"
    )
    
    parser.add_argument(
        "--generate-report", "-r",
        action="store_true",
        help="ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š"
    )
    
    parser.add_argument(
        "--save-history",
        action="store_true",
        help="ä¿å­˜å†å²ç»“æœ"
    )
    
    parser.add_argument(
        "--show-problems", "-p",
        action="store_true",
        help="æ˜¾ç¤ºé—®é¢˜æŸ¥è¯¢"
    )
    
    parser.add_argument(
        "--debug",
        action="store_true",
        help="å¯ç”¨è°ƒè¯•æ¨¡å¼"
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    setup_logging(args.debug)
    
    # è¿è¡Œè¯„ä¼°
    success = run_evaluation(args)
    
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main() 