# -*- coding: utf-8 -*-
"""
æµ‹è¯•è·¯å¾„åŒ¹é…ä¿®å¤çš„ç®€å•è„šæœ¬
"""

import sys
import os
sys.path.append(os.path.dirname(__file__))

from utils.metrics import EvaluationMetrics

def print_separator(title):
    """æ‰“å°åˆ†éš”çº¿å’Œæ ‡é¢˜"""
    print("\n" + "="*60)
    print(f"ğŸ“Š {title}")
    print("="*60)

def explain_path_matching_result(result):
    """è¯¦ç»†è§£é‡Šè·¯å¾„åŒ¹é…ç»“æœ"""
    print("\nğŸ” è·¯å¾„åŒ¹é…è¯¦ç»†åˆ†æ:")
    print(f"  ğŸ“ˆ æ€»åŒ¹é…åˆ†æ•°: {result['total_score']:.3f}")
    print("     â”œâ”€ åˆ†æ•°èŒƒå›´: 0.0-1.0 (è¶Šé«˜è¶Šå¥½)")
    print("     â””â”€ è®¡ç®—å…¬å¼: (ç²¾ç¡®åŒ¹é…Ã—1.0 + éƒ¨åˆ†åŒ¹é…Ã—0.7 + æ‰©å±•ååŒ¹é…Ã—0.3) / æœŸæœ›ç»“æœæ•°")
    
    print(f"\n  âœ… ç²¾ç¡®åŒ¹é…: {result['exact_matches']} ä¸ª")
    print("     â””â”€ æ–‡ä»¶è·¯å¾„å®Œå…¨ç›¸åŒ")
    
    print(f"  ğŸ¯ éƒ¨åˆ†åŒ¹é…: {result['partial_matches']} ä¸ª") 
    print("     â””â”€ æ–‡ä»¶åç›¸åŒä½†è·¯å¾„ä¸åŒï¼Œæˆ–è·¯å¾„æœ‰é‡å éƒ¨åˆ†")
    
    print(f"  ğŸ“„ æ‰©å±•ååŒ¹é…: {result['extension_matches']} ä¸ª")
    print("     â””â”€ æ–‡ä»¶æ‰©å±•åç›¸åŒä½†æ–‡ä»¶åä¸åŒ")
    
    print(f"  ğŸ“‹ æœŸæœ›ç»“æœæ€»æ•°: {result['total_expected']} ä¸ª")

def explain_new_framework_metrics(framework_metrics):
    """è¯¦ç»†è§£é‡Šæ–°è¯„ä¼°æ¡†æ¶æŒ‡æ ‡"""
    print("\nğŸ¯ æ–°è¯„ä¼°æ¡†æ¶åˆ†æ:")
    
    details = framework_metrics.get("details", {})
    weights = details.get("weights", {})
    
    print(f"  ğŸ† ç»¼åˆè¯„åˆ†: {framework_metrics['total_score']:.3f}")
    print("     â”œâ”€ è®¡ç®—å…¬å¼: ç›¸å…³æ€§Ã—0.5 + å…¨é¢æ€§Ã—0.3 + å¯ç”¨æ€§Ã—0.2")
    print(f"     â””â”€ è¯„ä¼°èŒƒå›´: å‰{details.get('k', 10)}ä¸ªç»“æœ")
    
    print(f"\n  ğŸ“Š ç›¸å…³æ€§ (æƒé‡{weights.get('relevance', 0.5)*100:.0f}%): {framework_metrics['relevance']:.3f}")
    print("     â”œâ”€ å«ä¹‰: å‰kä¸ªç»“æœä¸­ç›¸å…³ç»“æœçš„æ¯”ä¾‹")
    print(f"     â”œâ”€ è®¡ç®—: {details.get('relevant_in_top_k', 0)} / {details.get('k', 10)}")
    if framework_metrics['relevance'] >= 0.7:
        print("     â””â”€ è¯„ä»·: ğŸŸ¢ ä¼˜ç§€ - è¿”å›çš„ç»“æœå¤§éƒ¨åˆ†éƒ½ç›¸å…³")
    elif framework_metrics['relevance'] >= 0.5:
        print("     â””â”€ è¯„ä»·: ğŸŸ¡ è‰¯å¥½ - è¿”å›ç»“æœæœ‰ä¸€å®šç›¸å…³æ€§")
    else:
        print("     â””â”€ è¯„ä»·: ğŸ”´ è¾ƒå·® - è¿”å›äº†å¤ªå¤šä¸ç›¸å…³ç»“æœ")
    
    print(f"\n  ğŸ“ˆ å…¨é¢æ€§ (æƒé‡{weights.get('completeness', 0.3)*100:.0f}%): {framework_metrics['completeness']:.3f}")
    print("     â”œâ”€ å«ä¹‰: æ‰¾åˆ°äº†å¤šå°‘æ¯”ä¾‹çš„ç›¸å…³ç»“æœ")
    print(f"     â”œâ”€ è®¡ç®—: {details.get('relevant_in_top_k', 0)} / {details.get('total_relevant', 0)}")
    if framework_metrics['completeness'] >= 0.7:
        print("     â””â”€ è¯„ä»·: ğŸŸ¢ ä¼˜ç§€ - æ‰¾åˆ°äº†å¤§éƒ¨åˆ†ç›¸å…³ç»“æœ")
    elif framework_metrics['completeness'] >= 0.5:
        print("     â””â”€ è¯„ä»·: ğŸŸ¡ è‰¯å¥½ - æ‰¾åˆ°äº†éƒ¨åˆ†ç›¸å…³ç»“æœ")
    else:
        print("     â””â”€ è¯„ä»·: ğŸ”´ è¾ƒå·® - é—æ¼äº†å¾ˆå¤šç›¸å…³ç»“æœ")
    
    print(f"\n  ğŸ¯ å¯ç”¨æ€§ (æƒé‡{weights.get('usability', 0.2)*100:.0f}%): {framework_metrics['usability']:.3f}")
    print("     â”œâ”€ å«ä¹‰: ç¬¬ä¸€ä¸ªç›¸å…³ç»“æœçš„æ’åè´¨é‡ (MRR)")
    print("     â”œâ”€ è®¡ç®—: ç¬¬ä¸€ä¸ªç›¸å…³ç»“æœæ’åçš„å€’æ•°")
    if framework_metrics['usability'] >= 0.7:
        print("     â””â”€ è¯„ä»·: ğŸŸ¢ ä¼˜ç§€ - ç›¸å…³ç»“æœæ’åå¾ˆé å‰")
    elif framework_metrics['usability'] >= 0.5:
        print("     â””â”€ è¯„ä»·: ğŸŸ¡ è‰¯å¥½ - ç›¸å…³ç»“æœæ’åè¾ƒé å‰")
    else:
        print("     â””â”€ è¯„ä»·: ğŸ”´ è¾ƒå·® - ç›¸å…³ç»“æœæ’åå¤ªé å")

def explain_traditional_metrics(precision, recall, f1):
    """è§£é‡Šä¼ ç»ŸæŒ‡æ ‡ (å¯¹æ¯”å‚è€ƒ)"""
    print("\nğŸ“ˆ ä¼ ç»ŸæŒ‡æ ‡å¯¹æ¯”:")
    
    print(f"  ğŸ“ ä¼ ç»Ÿç²¾ç¡®ç‡: {precision:.3f}")
    print(f"  ğŸª ä¼ ç»Ÿå¬å›ç‡: {recall:.3f}")
    print(f"  âš–ï¸ ä¼ ç»ŸF1åˆ†æ•°: {f1:.3f}")
    print("     â””â”€ æ³¨: ä¼ ç»ŸæŒ‡æ ‡åŸºäºæ‰€æœ‰æ£€ç´¢ç»“æœï¼Œæ–°æ¡†æ¶åŸºäºå‰kä¸ªç»“æœ")

def test_path_matching():
    """æµ‹è¯•è·¯å¾„åŒ¹é…åŠŸèƒ½"""
    print_separator("ä»£ç æ£€ç´¢è¯„ä¼°ç»“æœåˆ†æ")
    
    print("ğŸ”„ æ­£åœ¨æµ‹è¯•è·¯å¾„åŒ¹é…åŠŸèƒ½...")
    
    # åˆ›å»ºè¯„ä¼°æŒ‡æ ‡å®ä¾‹
    metrics = EvaluationMetrics()
    
    # æ¨¡æ‹ŸAPIè¿”å›ç»“æœ
    actual_results = [
        {"path": "pages\\points\\exchange.vue", "score": 0.8},
        {"path": "pages\\coupon\\list.vue", "score": 0.6},
        {"path": "api\\goods.js", "score": 0.4}
    ]
    
    # æœŸæœ›ç»“æœ
    expected_results = [
        {"path": "pages\\points\\exchange.vue", "relevance_score": 1.0},
        {"path": "pages\\settlement\\style.scss", "relevance_score": 0.5}
    ]
    
    print("\nğŸ“‹ æµ‹è¯•æ•°æ®è¯´æ˜:")
    print("  ğŸ” æŸ¥è¯¢: 'ç§¯åˆ†å•†å“åˆ—è¡¨æ ·å¼'")
    print("  ğŸ“¤ APIè¿”å›ç»“æœ:")
    for i, result in enumerate(actual_results, 1):
        print(f"    {i}. {result['path']} (åˆ†æ•°: {result['score']})")
    
    print("  ğŸ¯ æœŸæœ›ç»“æœ:")
    for i, result in enumerate(expected_results, 1):
        print(f"    {i}. {result['path']} (ç›¸å…³æ€§: {result['relevance_score']})")
    
    # æµ‹è¯•é…ç½®æ ¼å¼çš„æƒé‡å‚æ•°ï¼ˆæ¨¡æ‹Ÿconfig.pyä¸­çš„æ ¼å¼ï¼‰
    config_weights = {
        "exact_match_weight": 1.0,
        "partial_match_weight": 0.7,
        "extension_match_weight": 0.3
    }
    
    try:
        # æµ‹è¯•è·¯å¾„åŒ¹é…
        result = metrics.calculate_path_matching_score(
            actual_results, 
            expected_results, 
            config_weights
        )
        
        print("\nâœ… è·¯å¾„åŒ¹é…è®¡ç®—æˆåŠŸ!")
        explain_path_matching_result(result)
        
        # æµ‹è¯•æ–°è¯„ä¼°æ¡†æ¶æŒ‡æ ‡
        framework_metrics = metrics.calculate_new_framework_metrics(actual_results, expected_results, k=10)
        explain_new_framework_metrics(framework_metrics)
        
        # æµ‹è¯•ä¼ ç»ŸæŒ‡æ ‡ (å¯¹æ¯”)
        precision, recall, f1 = metrics.calculate_precision_recall_f1(actual_results, expected_results)
        explain_traditional_metrics(precision, recall, f1)
        
        # é¢å¤–çš„å»ºè®®
        print_separator("æ”¹è¿›å»ºè®®")
        
        total_score = framework_metrics['total_score']
        if total_score < 0.6:
            print("ğŸ”§ æ”¹è¿›å»ºè®®:")
            if framework_metrics['relevance'] < 0.5:
                print("  â€¢ ç›¸å…³æ€§è¾ƒä½: å‰kä¸ªç»“æœä¸­ç›¸å…³ç»“æœå¤ªå°‘")
                print("  â€¢ å»ºè®®: æé«˜åŒ¹é…é˜ˆå€¼æˆ–æ”¹è¿›ç›¸ä¼¼åº¦è®¡ç®—")
            
            if framework_metrics['completeness'] < 0.5:
                print("  â€¢ å…¨é¢æ€§ä¸è¶³: é—æ¼äº†å¤ªå¤šç›¸å…³ç»“æœ")
                print("  â€¢ å»ºè®®: æ‰©å±•æœç´¢èŒƒå›´æˆ–ä½¿ç”¨åŒä¹‰è¯åŒ¹é…")
            
            if framework_metrics['usability'] < 0.5:
                print("  â€¢ å¯ç”¨æ€§è¾ƒå·®: ç›¸å…³ç»“æœæ’åå¤ªé å")
                print("  â€¢ å»ºè®®: ä¼˜åŒ–æ’åºç®—æ³•æˆ–è°ƒæ•´ç›¸å…³æ€§æƒé‡")
        
        if result['exact_matches'] == 0:
            print("  â€¢ æ²¡æœ‰ç²¾ç¡®åŒ¹é…: æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ ¼å¼æ˜¯å¦ä¸€è‡´")
            print("  â€¢ å»ºè®®: ç»Ÿä¸€è·¯å¾„åˆ†éš”ç¬¦ï¼ˆ\\æˆ–/ï¼‰å’Œå¤§å°å†™")
        
        print("\nğŸ‰ æµ‹è¯•å®Œæˆ! ç³»ç»Ÿè¿è¡Œæ­£å¸¸")
        return True
        
    except Exception as e:
        print(f"âŒ è·¯å¾„åŒ¹é…æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_path_matching()
    if success:
        print_separator("æ€»ç»“")
        print("ğŸ‰ ä¿®å¤éªŒè¯æˆåŠŸ!")
        print("âœ… exact_match é”®é”™è¯¯å·²è§£å†³")
        print("âœ… è¯„ä¼°ç³»ç»Ÿå¯ä»¥æ­£å¸¸ä½¿ç”¨")
        print("\nğŸ“š æ¥ä¸‹æ¥æ‚¨å¯ä»¥:")
        print("  1. è¿è¡Œå®Œæ•´è¯„ä¼°: python run_evaluation.py")
        print("  2. è¿è¡Œå¿«é€Ÿæµ‹è¯•: python quick_start.py")
        print("  3. æŸ¥çœ‹è¯¦ç»†æ–‡æ¡£: USAGE.md")
    else:
        print("\nâŒ ä¿®å¤éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯") 